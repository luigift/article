\relax 
\citation{dai2006prospects}
\citation{tabletop}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces General achitecture of the system}}{1}}
\newlabel{fig:architecture}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}PROPOSED APPROACH}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}General archtecture}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Object Segmentation}{1}}
\citation{caron2014neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}Feature}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-D}Aspect-Graph}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Représentation des objets par un modèle polaire}}{2}}
\newlabel{fig:graphe_polaire}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-E}Object representation}{2}}
\newlabel{sec:rec_mono}{{II-E}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-E.1}Graphe d'aspect polaire}{2}}
\citation{rusu2009fast}
\citation{tombari2010unique}
\citation{aldoma2011cad}
\citation{aldoma2012our}
\citation{Aldoma2012}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-E.2}Descripteurs}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-E.3}Reconnaissance mono-vue}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {reconnaissance mono-vue} - Résultat de la classification à partir d'une seule vue sur les objets segmentés comprenant un écran et un ventilateur. En rouge, le plan du sol et en blanc les points à plus de 3 mètre, non pris en compte dans la segmentation. On remarque également les ombres infra-rouges qui occultent les objets}}{3}}
\newlabel{fig:mono_recon}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Représentation des différents repères utilisés}}{3}}
\newlabel{fig:reperes}{{4}{3}}
\newlabel{eq:chi-square}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-F}Object localization and tracking}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-F.1}Définition des repères}{3}}
\newlabel{eq:mat_rotation}{{2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-F.2}Bases mobiles}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-F.3}Estimation de l'odométrie}{4}}
\newlabel{eq:integ}{{4}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-F.4}Filtre de Kalman }{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-F.5}Prédiction}{4}}
\newlabel{eq:kalman_prediction}{{5}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-F.6}Innovation}{4}}
\newlabel{eq:innovation}{{6}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-F.7}Suivi multi-cibles}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-G}Reconnaissance Multi-vue}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-G.1}Chaînes de Markov Cachées}{4}}
\newlabel{eq:delta_ang}{{7}{5}}
\newlabel{eq:dist_norm}{{8}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-G.2}Algorithme de Viterbi}{5}}
\newlabel{eq:viterbi}{{9}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experimental results}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Object Database}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Performance testing}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Multiple object recognition}{5}}
\bibstyle{plain}
\bibdata{rapport}
\bibcite{Aldoma2012}{1}
\bibcite{aldoma2012our}{2}
\bibcite{aldoma2011cad}{3}
\bibcite{caron2014neural}{4}
\bibcite{dai2006prospects}{5}
\bibcite{tabletop}{6}
\bibcite{rusu2009fast}{7}
\bibcite{tombari2010unique}{8}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {III-C.1}Odometry Contribution}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-D}Suivi et reconnaissance multi-cibles}{6}}
\@writefile{toc}{\contentsline {section}{References}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Expriment typique} - Reconnaissance multi-vue corrige des ambiguïtés malgré la mauvaise segmentation lors de la création de la base.}}{7}}
\newlabel{fig:resultats_expe}{{5}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Résultat de l'évaluation} - Les courbes en pointillés représentent la reconnaissance basée sur une seule image (mono-vue), tandis que les courbes pleines correspondent au système multi-vues. Avec un nombre réduit d'observations, la reconnaissance mono-vue tend à être légèrement plus performante. L'écart se creuse à mesure que ne nombre d'observations augmentent, jusqu'à un écart de 33 \% une fois le tour complet de l'objet effectué, soit 92\% de réussite pour l'estimation de l'objet et 74 \% pour l'estimation de l'orientation. }}{7}}
\newlabel{fig:comp}{{6}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Résultat du suivi multi-cibles} - Les points rouges correspondent aux centroïdes des objets segmentés. Chaque croix de couleur représente une estimation de position d'un filtre de Kalman, avec une couleur par filtre. En théorie, un filtre devrait être associé à chaque objet. En noir la trajectoire du robot au cours du temps.}}{8}}
\newlabel{fig:multi_map}{{7}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Reconnaissance Multi-cible} - Quatre des cinq objets présents dans la scène ont été correctement reconnus avec une estimation d'orientation raisonnable. Le première objet (une personne) était trop près du ventilateur, les deux ont donc été confondus dans le multi-tracking. }}{9}}
\newlabel{fig:recon}{{8}{9}}

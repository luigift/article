\relax 
\citation{krizhevsky2012imagenet}
\citation{kragic2009object,ali2014contextual}
\citation{caron2014neural}
\citation{bjorkman2004combination}
\citation{roy2000isolated,meger2008curious}
\citation{wang2007simultaneous}
\citation{kragic2009object}
\citation{tabletop}
\citation{caron2014neural}
\citation{csurka2004visual}
\citation{zhou2014learning}
\citation{tombari2010unique}
\citation{aldoma2012our}
\citation{rusu2009fast,rusu2010fast}
\citation{roy2004active}
\citation{bulthoff2002view}
\citation{aldoma2011cad}
\citation{chakravarty1982characteristic}
\citation{paletta2000active}
\citation{banta2000next}
\citation{le2014global}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}}
\citation{caron2014neural}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces General achitecture of the system}}{2}}
\newlabel{fig:architecture}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}PROPOSED APPROACH}{2}}
\newlabel{}{{II}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Overview}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Object Segmentation}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}Object Representation}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-C.1}Features}{2}}
\citation{rusu2010fast}
\citation{Aldoma2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Multi-view object reprensentation based on a polar aspect graph}}{3}}
\newlabel{fig:graphe_polaire}{{2}{3}}
\newlabel{sec:aspectgraph}{{II-C.2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-C.2}Multi-view object representation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-D}Object Recognition based on multiple views and their relative positions}{3}}
\newlabel{sec:singleviewpoint}{{II-D.1}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-D.1}Viewpoint recognition based on a single observation}{3}}
\newlabel{eq:chi-square}{{1}{3}}
\newlabel{eq:chi-prob}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Result of object segmentation and identification} The red points represent the floor plane and the ignored white ones exceed the three meters threshold. One could also notice the infra-red shadows created by objects, potentially leading to occlusions}}{3}}
\newlabel{fig:mono_recon}{{3}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-D.2}Object Tracking and Angular Displacement Estimation}{3}}
\citation{le2014global}
\citation{rabiner1989tutorial}
\citation{forney1973viterbi}
\newlabel{eq:triang}{{3}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-D.3}HMM filtering}{4}}
\newlabel{eq:transmat1}{{4}{4}}
\newlabel{eq:transmat}{{5}{4}}
\newlabel{eq:emismat}{{6}{4}}
\newlabel{eq:viterbi1}{{7}{4}}
\newlabel{eq:viterbi}{{8}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Experimental results}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Typical experiment} - The HMM refinement makes self correction of irrelevant views estimation, even when objects were badly segmented during the database creation.}}{5}}
\newlabel{fig:resultats_expe}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Object Database}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Odometry Contribution}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Numerical results evaluation} - Evolution of the correct recognition rate versus the number of views. Both for objects and view recognition, the multi-view approach outperforms the mono-view approach.}}{5}}
\newlabel{fig:comp}{{5}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Multi-object recognition}{5}}
\bibstyle{plain}
\bibdata{rapport}
\bibcite{Aldoma2012}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Mutli-objects tracking results} - Red dots corresponds to segmented objects centroids. Each color cross stands for the position estimation from a tracker. Each filter is associated with a unique color. Theoretically, a single filter should be associated with a single object. Black dots represent the trajectory of the robot during the experiment.}}{6}}
\newlabel{fig:multi_map}{{6}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusion}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Example of multi-view recognition in a scene with five objects - } For this run, odometry-based recognition got 80\% of the objects and 42 \% of the views right, the mono-view got 34\% for both, and the multi got 80\% right for object classification}}{6}}
\newlabel{fig:recon}{{7}{6}}
\bibcite{aldoma2012our}{2}
\bibcite{aldoma2011cad}{3}
\bibcite{ali2014contextual}{4}
\bibcite{banta2000next}{5}
\bibcite{bjorkman2004combination}{6}
\bibcite{bulthoff2002view}{7}
\bibcite{caron2014neural}{8}
\bibcite{chakravarty1982characteristic}{9}
\bibcite{csurka2004visual}{10}
\bibcite{tabletop}{11}
\bibcite{forney1973viterbi}{12}
\bibcite{kragic2009object}{13}
\bibcite{krizhevsky2012imagenet}{14}
\bibcite{le2014global}{15}
\bibcite{meger2008curious}{16}
\bibcite{paletta2000active}{17}
\bibcite{rabiner1989tutorial}{18}
\bibcite{roy2000isolated}{19}
\bibcite{roy2004active}{20}
\bibcite{rusu2009fast}{21}
\bibcite{rusu2010fast}{22}
\bibcite{tombari2010unique}{23}
\bibcite{wang2007simultaneous}{24}
\bibcite{zhou2014learning}{25}
\@writefile{toc}{\contentsline {section}{References}{7}}

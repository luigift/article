%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

%\usepackage{graphics} % for pdf, bitmapped graphics files
% The following packages can be found on http:\\www.ctan.org
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage{latexsym}
%\usepackage{amsthm}
\usepackage{fancyhdr}
%\usepackage{algorithmic}
\usepackage{cite}
\usepackage{array}

\title{\LARGE \bf
An Odometry-Based Reinforcement Approach for Indoor Objects and Viewpoint Recognition
}


\author{Luigi F. Tedesco$^{1}$, C\'eline Craye$^{2}$, Jean-Fran\c{c}ois Goudou$^{3}$ and David Filliat$^{4}$% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Luigi Franco Tedesco is an undergraduate student with Faculty of Electrical Engineering, Robotics and Artificial Intelligence,
        Thales Service, 91767 Palaiseau, France
        {\tt\small tedesco@ensta.fr}}%
\thanks{$^{2}$Bernard D. Researcher is with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Object recognition capability is a essential condition for giving autonomy to mobiles robots in human made environment. However, achieving this goal by means of visually representing objects is a hard task \ref{} and using all possible sources of information is a must. Here we present a procedure to incorporate the notion of continuity and overcome ambiguous points of view. By observing objects from different perspectives binded with a Markovian modeling of the stochastic processes of recognizing each of the objects viewpoint, the algorithm copes with a sparse database, blurred images from motion and object spatial symmetry, to recognize and estimate objects 6-dof pose. A multi-modal Kalman based tracking was also implemented in order to recognize multiple objects simultaneously. The approach was tested in a mobile platform and the comparison between the single viewed and the proposed recognition gave promising results. 

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

 The vast majority of the literature focus on single image object visual recognition for helping robots in tasks such as semantic navigation \ref{}, pose estimation for grasping \ref{} and environmental search \ref{}. Typically, a set of features is extracted from a segmented object candidate and, subsequently, compared to a database of priori known objects. Extensive work have been done in order to increase efficiency in each one of the sub-processing steps. Among them : segmentations methods using range cameras, features that describe color and texture \ref{}, geometry \ref{}, contours \ref{}, besides classifiers and matching techniques. Alternatively, a deep neural architecture \ref{} can perform a direct object visual classification after a delicate training phase. However, the classic recognition pipeline seems to be more natural and simple to be implemented with a straight-forward training, still having reasonable results.
 
 Nevertheless, ambiguous viewpoints easily trick visual descriptors reducing its recognition capability. Observing objects sequentially from distinctive points of view seems to be a natural way to deal with the problem. A solution inspired by human behavior for learning new unseen objects has been proposed by \ref{}, using key-frames and the rate of matching features with past frames, to overcome ambiguity in face recognition task. More work have been done to model objects different viewpoints perspectives summarized by Roy and al. \ref{}. 
 
\section{PROPOSED APPROACH}

\subsection{General archtecture}

Our approach was designed for robots equipped with odometric sensors and an RGB-D camera. Informations from these units are sent to a processing module that isolates objetcs from input images extracts features that describe each objetc, and compared them with a reference database stored in memory. When no matching is found between observation and the database, a new object can be added to the database and increase the robot's knowledge about the environment.

The architecture of the system is illustrated in Figure \ref{fig:architecture} and explains the dependencies between the different processing modules and the input and output flow for each of them.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{gen_arc.png}
  \caption{General achitecture of the system}
  \label{fig:architecture}
\end{figure}

More precisely, the processinng units takes as an input the point cloud from the RGB-D sensor as well as the odometry measure of the two wheels. The first module consists in a segmentation step that cleans up the point cloud by isolationg objects of the scene. A new point cloud for each object is then created. Those point clouds are then sent to the feature extraction module to convert them to discriminative feature histograms. On the other hand, another module converts the odometry data to a localization and displacement of the robot in an absolute reference. Each segmented object can then be localize in the same reference and sent to the recognition module. The recognition module uses both the object feature and the evolution of those features compared to the displacement of the robot to retrieve the most likely object and view.


\subsection{Object Segmentation}

The segmentation step aims to differentiate objects from the background of raw images. Stereoscopic and infra-red cameras helped the treatment adding a new dimension to images and allowing segmentation geometrically. In the case where the sensor is motionless, background subtraction approach are typically used for segmentaiton \cite{dai2006prospects}. This is not applicable in our case as the robot is constantly moving in its environment. By making the hypothesis that objects are represented in the scene by as cluster of points right above the ground plan, other approaches such as \textit{Tabletop object detector} \cite{tabletop} determine the main plane of the image from depth data and search for elements in the convex hull of the plane that are not the plane itself. For indoor applications, this assumption is usually enough to isolates objects on tables or floor. 

The segmentation algorithm used in this work is the one proposed by  Caron et al. \cite{caron2014neural}. The approach uses the same assumption that objects are lying on planes surfaces that can be found as the major plane of images. In our case, as the camera on the robot is always keeping the same orientation towars the floor, the floor plane equation can be fond during a calibration step and used during the whole experiment. The segmentation algorithm uses the following steps

\begin{enumerate}[start=0]
\item Calibration to obtain the floor plane equation before experiments, using RANSAC algoritm on an image where floor is the main plane.
\end{enumerate}
 Then for each frame of the experiment:
\begin{enumerate}
\item Floor subtraction from the obtained equation
\item Points filtering if the distance to the sensor is more than 3 meters
\item Normal surfaces of the remaining points calculation
\item Filtering of big planes orthogonal to the ground. They are very likely to be walls
\item Voxelization of remaining points to speed up processing
\item Projection of those points on the floor plane
\item Clustering of the projected points to create object candidates
\item Elimination of clusters that are too close to borders. They might not be objects, and their missing parts are likely to badly influence the recognition.
\item Centroid calculation of each remaining cluster (should be the objects of the image)
\end{enumerate}

Based on this algorithm, we obtain the position of each object in the camera frame, as well as the associated point cloud and normals.

\subsection{Feature}

Among all kind of image features, the Viewpoint Feature Histogram captures the object geometry by estimating the angular transformation between the normal of each of the object's point and the standpoint from where it has been viewed. The interest of using such a feature is to explore the ambiguity created from objects spatial symmetry.

\subsection{Aspect-Graph}

In order to represent objects in the 3 dimensional space, an aspect graph representation merges viewpoint appearance and the necessary movement to transit between them. 

\subsection{Object representation}

Among all kind of image features, the Viewpoint Feature Histogram captures the object geometry by estimating the angular transformation between the normal of each of the object's point and the standpoint from where it has been viewed. The interest of using such a feature is to explore the ambiguity created from objects spatial symmetry.

L'objectif de notre méthode est d'avoir une reconnaissance multi-vues d'un ou plusieurs objets à la fois, capable d'intégrer le déplacement du robot pour résoudre des ambiguïtés et faux positifs. Pour incorporer les notions de vues et de transition entre elles, on utilise une représentation simple et suffisamment générale basée sur les graphes d'aspect. Le déplacement d'un état à un autre dans ce graphe est ensuite estimé par rapport au déplacement du robot. Ce système est ensuite couplé avec un dispositif de reconnaissance mono-vue classique capable de retrouver la vue la plus probable d'un objet à partir de descripteurs 3D. Une méthode de suivi des objets et un traitement probabiliste de changement de vue étant donné l'information motrice permet enfin d'augmenter le taux de reconnaissance.

\label{sec:rec_mono}
Afin de reconnaitre les objets et leurs points de vue rencontrés par le robot, nous utilisons une base de données réalisée à l'avance. Dans cette base, des descripteurs de plusieurs objets sont calculés pour plusieurs points de vues ainsi que leurs positions relatives (Plus de détails sur la construction de la base à la section \ref{sec:base_donnees}). Afin de stocker et accéder aux données des objets de la base en mémoire, nous utilisons une représentation basée sur un graphe d'aspect polaire.

\subsubsection {Graphe d'aspect polaire}
%\celine{Je trouve que cette section n'est pas très utile en fait. Tu fais référence ailleurs à un graphe d'aspect (je ne sais plus où). Si tu veux, tu peux mettre cette partie dans les annexes et y faire référence à l'endroit du rapport ou tu en parles}

On considère que les objets sont décrits par deux dimensions d'information : une spatiale, représentant la position absolue de l'objet dans l'environnement ainsi que les positions relatives où l'objet a été visualisé, et une autre dimension visuelle, donnée par les descripteurs géométriques, de couleurs et de texture. On cherche à représenter cette description dans un référentiel unique. Le graphe d'aspect permet de coupler l'ensemble des images suivant ses possibles transitions spatiales, ce qui résulte dans la possibilité de construire le modèle à la volée et de jouer avec sa densité d'information - le nombre d'images intégrées au modèle.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{object_model.png}
  \caption{Représentation des objets par un modèle polaire}
	\label{fig:graphe_polaire}
\end{figure}

Un référentiel polaire permet d'intégrer toutes ces informations de façon à représenter la position spatiale d'où l'observation a été faite, comme représenté dans l'image \ref{fig:graphe_polaire}. Pour la construction du modèle les conventions suivantes ont été adoptées :
\begin{itemize}
\item l'angle zéro est attribué à la première observation
\item L'origine du référentiel est la position globale de l’objet
\item Les features sont labellisées d'après le déplacement angulaire
  et la distance au centroïde de l'objet.
\end{itemize}

Une grande majorité des features visuelles ne sont pas invariantes à l'échelle, et ce d'autant plus si la résolution de l’image joue un rôle critique pour la
détection de features, comme les patches SIFTs. Ainsi, prendre également en compte la distance à laquelle l’image a été prise peut être intéressant pour limiter la
classification à une échelle valable.

\subsubsection{Descripteurs}

Le travail des descripteurs est, d'une part, d'extraire des caractéristiques intéressants de l'élément observé et, d'autre part, de réduire la
dimensionnalité de l'espace traité, tout en restant robuste à des transformations affines et aux changements de luminosité. On s'intéresse surtout ici aux descripteurs basés sur le nuage de points des objets, bien qu'il soit possible aussi d'utiliser des descripteurs associés à la texture ou à la couleur. Les descripteurs qui nous intéressent sont des descripteurs géométriques qui essaient de traduire les idées de courbure, de forme et taille dans les histogrammes, et sont intéressants pour étudier les ambiguïtés de reconnaissance. Parmis les descripteurs 3D proposés dans la littérature, on peut citer FPFH \cite{rusu2009fast} qui est invariant par changement de point de vue, SHOT \cite{tombari2010unique} qui est un descripteur local de courbure et des descripteurs semi-globaux dédiés au traitement des occlusions, CVFH \cite{aldoma2011cad} et Our-CVFH \cite{aldoma2012our}. Une description détaillée de ces descripteurs et leurs principales différences sont expliquées dans les annexes \ref{annexe:descripteur}. Nous choisissons d'utiliser le descripteur \textit{Viewpoint Feature Histogram} - VFH, car il permet de discriminer non seulement les formes géométriques (pour la reconnaissance d'objet), mais aussi les points de vues (reconnaissance de vue).

En partant du principe que la segmentation propose un découpage correct des objets, on extrait des descripteurs globaux à partir des ensembles de points proposés. Ainsi, pour chaque objet segmenté, on obtient un histogramme VFH représentatif de l'objet et de sa vue courante. D'autre part, les histogrammes VFH de tous les éléments de la base de données sont calculés au préalable pour être comparés plus rapidement.

\subsubsection{Reconnaissance mono-vue}
Le but est de retrouver l'objet et son point de vue le plus proche par rapport à la base. Pour cela, il est possible d'utiliser des algorithmes de \textit{machine learning} classiques, mais les résultats obtenus avec des réseaux de neurones n'ont pas été très concluants. Aldoma et al. \cite{Aldoma2012} suggèrent l'utilisation de la mesure de similarité entre histogrammes chi-squared, associée au classificateur \textit{k plus proches voisins}, ou K-NN. Le gros avantage de ce classificateur est l'étape d’apprentissage, qui correspond à création d’un arbre de recherche construit à partir de la comparaison croisée entre les éléments de la base. Par rapport aux données dont nous disposons, cet arbre se construit et fournit une estimation du plus proche voisin de manière presque instantannée.

L'API de la librairie FLANN sur PCL permet l'utilisation directe du classificateur K-NN. L'implémentation permet l'utilisation de plusieurs définitions de distance entre histogrammes. La définition par défaut, Chi-squared, dont la formule est donnée à l'équation \ref{eq:chi-square}, semble être capable de bien différencier les histogrammes d'entrés, $H_1$ et $H_2$, et a été choisie pour notre système.
\begin{equation}
d(H_1, H_2) = \sum _I \frac{\left(H_1(I)-H_2(I)\right)^2}{H_1(I)}
\label{eq:chi-square}
\end{equation}

\begin{figure}
	\includegraphics[width=6cm]{mono_recon.png}
	\caption{\textbf{reconnaissance mono-vue} - Résultat de la classification à partir d'une seule vue sur les objets segmentés comprenant un écran et un ventilateur. En rouge, le plan du sol et en blanc les points à plus de 3 mètre, non pris en compte dans la segmentation. On remarque également les ombres infra-rouges qui occultent les objets}
	\label{fig:mono_recon}   
\end{figure}

\subsection{Object localization and tracking}

\subsubsection{Définition des repères}

Se placer dans différents repères permet d'avoir des référentiels plus naturels pour chaque type de composant du robot et pour les objets placés dans la scène. On définit quelques repères et conventions de base pour faciliter la localisation. Tout d'abord, le repère de la base du robot est orthonormal positif, où le déplacement vers l'avant correspond à l'axe $\vec{x}$, vers la gauche à l'axe $\vec{y}$ et vers le haut à l'axe $\vec{z}$. Le repère monde (ou repère absolu) est choisi comme étant le repère du robot dans sa position initiale. Un troisième référentiel utilisant les mêmes conventions positionne le capteur RGB-D par rapport au robot. Enfin, le dernier référentiel correspond au repère optique du capteur orienté selon la convention usuelle pour les images avec l'axe $\vec{x}$ orienté vers la droite, l'axe $\vec{y}$ vers le bas et enfin l'axe $\vec{z}$ vers l'avant. Ces trois repères permettent d'orienter tous les éléments aperçus par le robot dans l'environnement de façon pratique.

La figure \ref{fig:reperes} permet de visualiser les différents repères utilisés.

\begin{figure}
  \centering
  \includegraphics[width=4cm]{reperes.png}
  \caption{Représentation des différents repères utilisés}
  \label{fig:reperes}
\end{figure}


Une méthode de transformation entre repères permet ensuite le passage de l'un à l'autre. On peut ainsi obtenir la position de l'objet  dans le repère global d'après sa détection par la caméra. La transformation entre une base $a$ et une autre $b$ est faite par une matrice de transformation classique, décrit par l'équation \ref{eq:mat_rotation}. 

\begin{equation}
	\mathbf{R}^{a}_{b} = 
	\begin{bmatrix} 
	 	\cos \theta &  -\sin \theta & \Delta x \\ \sin \theta & \cos \theta & \Delta y \\ 0 & 0 & 1
	 \end{bmatrix}
	\label{eq:mat_rotation}
\end{equation}

où $\theta$ équivaut à l'angle entre les deux repères et $\Delta x$ et $\Delta y$ sont les translations entre les deux origines.

\subsubsection{Bases mobiles}

\subsubsection{Estimation de l'odométrie}

Certains robots sont dotés de capteurs capables d'estimer de façon
approximative leurs déplacements. C'est aussi le cas du robot utilisé qui
possède des roues codeuses capables d'estimer la rotation angulaire des
roues. Pour le cas d'un robot différentiel, où chaque roue peut être
commandée indépendamment, le déplacement et l'orientation suivent les équations suivantes:

\begin{equation}
	\begin{array}{rcl}
		\delta x_t &=& \delta s_t \cdot cos(\theta_{t-1}) \\
		\delta y_t &=& \delta s_t \cdot sin(\theta_{t-1}) \\
		\delta \theta_t &=& \frac{\delta \omega_{g} + \delta \omega_{d}}{d_{r}}\\
		\delta s_t &=& \frac{\delta \omega_{g} + \delta \omega_{d}}{2}						
	\end{array}
\end{equation}
$\omega_g$ et $\omega_d$ sont des respectives variations angulaire des roues droites et gauches, et $d_r$ est la distance entre elles. Une intégration, au sens mathématique, de l'odométrie entre deux intervalles de temps permet de retrouver la position global du robot par l'équation \ref{eq:integ}:

\begin{equation}
	\begin{array}{rcl}
		x_t &=& x_{t-1} + \delta x_{t} \times cos(\theta_{t-1}) - \delta y_{t} \times sin(\theta_{t-1}) \\
		y_t &=& y_{t-1} + \delta x_{t} \times sin(\theta_{t-1}) + \delta y_{t} \times cos(\theta_{t-1}) \\
		\theta_t &=& \theta_{t-1} + \delta\theta_{t}
	\end{array}
	\label{eq:integ}
\end{equation}

\subsubsection{Filtre de Kalman }

Afin de pouvoir utiliser le déplacement du robot par rapport aux objets pour aider leur identification, il est d'abord nécessaire de les localiser et les suivre. À cause de la divergence de l'odométrie, l'imprécision de la segmentation et le calcul du centroïde de l'objet, la position estimée est fortement bruitée et rend le suivie et identification infaisables lorsque les objets sont trop proches. Nous utilisons donc un filtre de Kalman pour corriger cette erreur de mesure et fournir une estimation plus fiable de la position des objets.

Classiquement le filtre de Kalman est mis à jour selon deux étapes : 

\subsubsection{Prédiction} Une première de prédiction qui utilise un modèle de dynamique linéaire $\textbf{F}_{k}$ pour décrire l'évolution des états au long du temps avec son bruit de process $\textbf{Q}_{k}$ associé, et qui estime \textit{a priori} la covariance de l'erreur $\textbf{P}_{k|k-1}$. Formellement, on utilise les équations \ref{eq:kalman_prediction}

\begin{equation}
	\begin{array}{ccl}
		\hat{\textbf{x}}_{k|k-1} &=& \textbf{F}_{k}\hat{\textbf{x}}_{k-1|k-1} + \textbf{B}_{k} \textbf{u}_{k-1}\\
		\textbf{P}_{k|k-1} &=& \textbf{F}_{k} \textbf{P}_{k-1|k-1} \textbf{F}_{k}^{T} + \textbf{Q}_{k}
	\end{array}
	\label{eq:kalman_prediction}
\end{equation}

\noindent Où les variables sont :\\
$\textbf{F}_{k}$ : la matrice de dynamique du système définie comme identité dans notre cas, si l'on considère que l'objet reste immobile\\
 $\textbf{u}_{k}$ : l'entrée de commande, nulle dans notre cas\\
 $\textbf{B}_{k}$ : la matrice qui relie l'entrée de commande $u$ à l'état $x$, nulle également\\
$\textbf{P}_{k|k-1}$ : la matrice d'estimation a priori de la covariance de l'erreur \\
$\textbf{Q}_{k}$ : la matrice de covariance du bruit de process, diagonale dans notre cas.\\

%\noindent Avec: \\
%$\textbf{z}_{k}$ \space \space: observation ou mesure du process à l'instant k \\
%$\textbf{H}_{k}$ \space : matrice qui relie l'état $\textbf{x}_{k}$ à la mesure$ \textbf{z}_{k}$\\
%$\textbf{P}_{k|k}$ : matrice d'estimation a posteriori de la covariance de l'erreur\\
%$\textbf{R}_{k}$ \space \space : matrice de covariance du bruit de mesure

\subsubsection{Innovation}
Une deuxième mise à jour, où l'observation est incorporée dans le calcul de l'innovation, $\tilde{\textbf{y}}_{k}$, et du gain de Kalman, $\textbf{K}_{k}$ est décrite par l'équation \ref{eq:innovation}.

\begin{equation}
	\begin{array}{ccl}
		\tilde{\textbf{y}}_{k} &=& \textbf{z}_{k} - \textbf{H}_{k}\hat{\textbf{x}}_{k|k-1} \\
		\textbf{S}_{k} &=& \textbf{H}_{k}\textbf{P}_{k|k-1} \textbf{H}_{k}^{T}+\textbf{R}_{k} \\
		\textbf{K}_{k} &=& \textbf{P}_{k|k-1}\textbf{H}_{k}^{T}\textbf{S}_{k}^{-1} \\
		\hat{\textbf{x}}_{k|k} &=& \hat{\textbf{x}}_{k|k-1} + \textbf{K}_{k}\tilde{\textbf{y}}_{k} \\
		\textbf{P}_{k|k} &=& (I - \textbf{K}_{k} \textbf{H}_{k}) \textbf{P}_{k|k-1}
	\end{array}
	\label{eq:innovation}
\end{equation}
\noindent Avec :\\
 $\textbf{z}_{k}$ : l'observation c'est-à-dire la position de l'objet segmenté dans le repère monde\\
$\textbf{H}_{k}$ : la matrice qui relie l'état $\textbf{x}_{k}$ à la mesure $ \textbf{z}_{k}$ : Ici, il s'agit d'une matrice identité puisque tout est représenté dans le repère monde.\\
$\textbf{P}_{k|k}$ : la matrice d'estimation \textit{a posteriori} de la covariance de l'erreur\\
$\textbf{R}_{k}$ : la matrice de covariance du bruit de mesure, matrice diagonale dans notre cas.

\subsubsection{Suivi multi-cibles}
Le caractère monomodal du filtre de Kalman nous contraint à ne pouvoir suivre qu'un seul objet à la fois. Pour obtenir un suivi multimodal, il faut que plusieurs filtres tournent en parallèle. Ainsi, le problème passe d’estimer la position d'un seul objet à celui de décider quelle observation appartient à quel filtre. Pour ce faire, nous définissions une matrice $M_{i,j}$ de distances entre chaque nouvelle observation $i$ et les états $j$ chaque filtre de Kalman déjà créé.
La mise à jour de chaque filtre s'effectue de la manière suivante : Pour chaque nouvelle observation $i$, on recherche le filtre dont l'état
Ensuite, les nouvelles observations sont utilisées pour mettre à jour les filtres de Kalman dont l'estimation est la plus proche selon cette matrice. Avant toute mise à jour, on vérifie que la distance entre l'observation et l'estimation du filtre ne dépasse pas un certain seuil. Pour chaque observation qui n'a pas pu être associée à un filtre déjà existant, on crée alors un nouveau filtre.

\subsection {Reconnaissance Multi-vue}

\subsubsection {Chaînes de Markov Cachées}

Le déplacement physique du robot produit une séquence
d'observations d'un même objet, sous différents points de vues. On exploite l'information odométrique entre les différentes vues pour renforcer l'estimation de la vue d'un objet. De cette manière, l'évolution de la reconnaissance au cours du temps est représentée par un processus stochastique, dont une modélisation possible consiste à le traiter de façon discrète dans un espace d'état. Ayant l'apriori que la dernière image et le dernier déplacement suffisent pour faire cette
prédiction (c'est-à-dire en respectant la propriété de Markov de premier ordre), le processus stochastique est modélisée par une chaîne de Markov cachée.

Concrètement, les états cachés correspondent à des vues d'objets présents dans la base de données et déjà stockés dans la mémoire du robot. Cela contraint le
nombre d'états et garantie que la chaîne soit finie. Puis, une matrice de transition $a_{i,j}$ décrit l'évolution du processus. C'est cette matrice de transition qui permet de prendre en compte l'odométrie et la transition entre les vues d'un même objets. Enfin, une autre matrice, $\mathrm{P}\big( y_1 \ | \ k \big)$, dite matrice d'émission, estime la vraisemblance entre l'observation et les états de la chaîne.

Plus précisément,  $a_{i,j}$ est définie en fonction de l'angle $\delta_{angle}$, calculé par \ref{eq:delta_ang} qu'a parcouru le robot par rapport à l'objet entre deux vues successives. Dans notre modèle, on considère que $a_{i,j}$ est nulle si $i$ et $j$ sont deux vues d'objets différents. Pour deux vues $i$ et $j$ d'un même objet, séparées d'une distance $d$, le poids accordé à $a_{i,j}$ sera d'autant plus fort que $\delta_{angle}$ et $d$ sont proches. D'autre part, la matrice d'émission $\mathrm{P}\big( y_1 \ | \ k \big)$ correspond à la similarité entre l'histogramme d'un objet segmenté $y$ et celui d'une vue d'objet dans la base de données $k$, normalisés par l'équation \ref{eq:dist_norm}. La similarité est calculée comme l'inverse de la distance Chi square définie à la section \ref{sec:rec_mono}.
  \begin{equation}
  \begin{array}{rcl}
    \vec{d}_0 &=& p_0 - p_{obj}\\
    \vec{d}_1 &=& p_1 - p_{obj} \\
     \delta_{angle} &=& atan(\vec{d}_1) - atan(\vec{d}_0)
  \end{array}
  \label{eq:delta_ang}
  \end{equation}
La transformation des distances des histogrammes en probabilité est faite d'après la normalisation suivant :
\begin{equation}
	\mathbb{P}(y|x, database) = \frac{\sum_a d_a^x - d_y^x}{\sum_b \sum_c d_c^x - d_b^x}
	\label{eq:dist_norm}
\end{equation}
où $x$ est l'image de test et y un élément de la base de données. Dans le cas du plus proches voisin, la normalisation ne prend en compte que les k plus proches histogrammes, par opposition à une approche \textit{brute force}.

Une autre modélisation possible aurait été d'avoir une chaîne de Markov
cachée distincte pour chaque objet et ensuite décider à chaque pas de temps le
processus le plus vraisemblable. Cette modélisation peut être vue comme un sous-ensemble du cas
précédent où les transitions entre deux objets ne sont pas
considérées. Pourtant, il peut arriver que deux objets soit considérés comme positionnés au même endroit, ou bien que des objets mobiles fusionnent (par exemple, une personne qui viendrait s'asseoir sur une chaise, ou encore une personne
qui commence à marcher) \footnote{Le fait de se mettre en mouvement
  altère les formes d'une personne, ce qui rend possible sa détection
  comme un nouvel objet.}.

\subsubsection{Algorithme de Viterbi}

Reste donc à extraire des informations de la modélisation Markovienne proposée.
La séquence d'états la plus vraisemblable qui pourrait avoir géneré
les observations  $y_1,\dots, y_T$, correspond normalement à la séquence d'objets reconnus.
Afin de retrouver cette séquence, aussi appellée chemin, on fait 
appel à la programmation dynamique, et plus spécifiquement à l'algorithme de Viterbi, d'où le nom chemin de Viterbi.
L'algorithme retrouve de façon récursive l'état courant le plus probable, en
prenant en compte seulement les observations jusqu'à un instant donné et son
estimation aux instants antérieurs. Ceci se traduit par les équations \ref{eq:viterbi}

\begin{equation}
  \begin{array}{rcl}
    V_{1,k} &=& \mathrm{P}\big( y_1 \ | \ k \big) \cdot \pi_k \\
    V_{t,k} &=& \max_{x \in S} \left(  \mathrm{P}\big( y_t \ | \ k \big) \cdot a_{x,k} \cdot V_{t-1,x}\right)
  \end{array}
	\label{eq:viterbi}
\end{equation}

Ici, $V_{t,k}$ représente la probabilité que la séquence d'états la plus probable finisse dans l'état $k$, ayant généré les observation à l'instant $t$, tandis que $\pi_i$ représente la probabilité initiale de se retrouver en chaque état. Pour retrouver le chemin de Viterbi, il suffit de trouver le maximum de $V_{t,k}$ :

\begin{equation}
  \begin{array}{rcl}
    x_T &=& \arg\max_{x \in S} (V_{T,x})
  \end{array}
\end{equation}


\section{Experimental results}
The proposed recognition system was deployed in a differential mobile robot, Wifibot V2, embedded with a RGB-D camera, Asus Xtion Pro Live. The algorithm architecture were implemented over ROS using PCL and OpenNi2 libraries. 
In the interest of validating the approach, the robot was initially taught objects aspects graphs and two sets of experiments were proposed to analyze the efficiency of the algorithm in real scenarios.

\subsection{Object Database}
First, twenty objects varying in size and form were selected to compose the robot knowledge database. The objects aspect graphs were composed by VFH features from eight equally distant viewpoints acquired from positioning the robot around the to be learn object 1.5 meters away. Each of the feature was labeled 


\subsection{Performance testing} 
The first experiment consist in a performance comparison between the single and multi image recognition techniques. In other words, this comparison attest whether the architecture is interesting or not, since having same performance by the cost of adding a complex post-processing and tracking modules is not interesting.
Tours around each of the known objects arranged in 4 different poses. 

\subsection{Multiple object recognition}
A second 

To evaluate the recognition capability of our system, we use the Wifibot V2, equiped with an embedded computer. The RGB-D image aquisition is obtained with an Asus Xtion Pro Live sensor. We use ROS environment to acquire and process the data both from the camera and the odometry.

For our evaluation, twenty objects of various size and shapes (monitor, boxes, chairs, trashcans, persons, ...) were used to create the database. For each of them we moved the robot to make a complete circle around them separately. Eights view were obtained from each circle, so that the angular distance between each of them was 45 degrees. The point cloud and associated features were extracted and recorded for each view.

\subsubsection{Odometry Contribution}
In a first evaluation, another complete circle was made around objects to evaluate the recognition capacity of the system. For each objects, we made four successive circles starting at different positions around the object. The robot was recording for each circle between 20 and 30 frames at different viewpoints of the object.

La difficulté de l'évaluation vient, premièrement, du fait que la base de donnée a été réalisée avec très peu de vues, ce qui donne lieu à de mauvaises reconnaissance mono-vue lorsque le point de vue observé se situe entre deux vues de la base de données. De plus, la vitesse de déplacement peut générer des images plus floues lors des acquisitions et la modification de l'angle de la caméra\footnote{L'angle entre la base et la tête de la caméra Asus Xtion est facilement modifié.} apportent un obstacle en plus pour le \textit{matching} de descripteurs dans le classificateur K-NN.

Un expérience typique est illustrée dans l'image \ref{fig:resultats_expe} :

\begin{figure*}
	\includegraphics[height=6cm]{hmm_example.png}
			\caption{\textbf{Expriment typique} - Reconnaissance multi-vue corrige des ambiguïtés malgré la mauvaise segmentation lors de la création de la base.}
	\label{fig:resultats_expe}
\end{figure*}

La première ligne correspond à la séquence d'images vues par le robot à chaque instant de temps, et donc, l'objet à être reconnu. La seconde ligne, donnée par l'algorithme de reconnaissance, équivaut à la vue la plus probable de l'objet reconnue par le K-plus proches voisins. Il est intéressant remarquer que l'invariance à rotation du descripteur trompe l'estimation de l'orientation en prenant son correspond énantiomorphe dans le premier carré rouge. Autrement, le dos du pinguin étant une grande surface presque plane, il est partiellement retiré par l'étape de segmentation. Ainsi, le nuage de points résultant de ce point de vue n'est pas suffisamment complet pour caractériser correctement l'objet, ce qui induit une mauvaise reconnaissance dans le carré bleu. Au final, on remarque que le traitement apporté par la chaîne de Markov cachée permet de corriger les problèmes d'une base de donnée relativement sparse avec des possibles erreus de segmentation, permettant la correction simultanée de la reconnaissance de l'objet et de son orientation en ligne. 

Le résultat de l'évaluation peut être représenté sous forme d'une courbe : le nombre d'observations pour un même objet en abscisse, par rapport aux pourcentage de reconnaissance d'objet et de vue, pour les reconnaissance mono et multi-vues.

\begin{figure*}
	\includegraphics[width=\textwidth]{comp.png}
	
	\caption{\textbf{Résultat de l'évaluation} - Les courbes en pointillés représentent la reconnaissance basée sur une seule image (mono-vue), tandis que les courbes pleines correspondent au système multi-vues. Avec un nombre réduit d'observations, la reconnaissance mono-vue tend à être légèrement plus performante. L'écart se creuse à mesure que ne nombre d'observations augmentent, jusqu'à un écart de 33 \% une fois le tour complet de l'objet effectué, soit 92\% de réussite pour l'estimation de l'objet et 74 \% pour l'estimation de l'orientation. }		
	\label{fig:comp}
\end{figure*}

La courbe de la figure \ref{fig:comp} permet de conclure, tout d'abord, que l'algorithme de reconnaissance multi-vues est plus performant que sa correspondante mono-vue lorsque le nombre d'observations augmente suffisamment.\footnote{Le fait que la reconnaissance mono-vue chute avec le nombre de vues peut être liée au fait que pour les premières secondes de l'expérience, le robot est immobile et correctement placé. Lors du déplacement du robot, les prises de vues sont globalement moins bonnes et font chuter le taux de reconnaissance.}. Dans un deuxième temps, on constate que l'estimation de l'orientation de l'objet est plus difficile à estimer que sa reconnaissance, que cela soit pour la reconnaissance mono-vue ou multi-vues.


%\subsection{Robustesse à l'occlusion}

\subsection{Suivi et reconnaissance multi-cibles}

La deuxième expérimentent consiste à placer des objets présents dans la base de données dans une pièce et conduire le robot en faisant en sorte qu'il les regarde  sous plusieurs points de vues différents. Ce scénario est beaucoup plus complexe que celui d'avant. D'abord les objets s'occultent les uns les autres, donnant lieux à de mauvaises segmentations. Ensuite, le suivi des objets est beaucoup plus complexe, car des objets proches peuvent être confondus.

La carte finale donnée par l'algorithme et localisant le robot ainsi que les objets dans le repère absolu est représentée à la figure \ref{fig:multi_map}. Une photo de la pièce aves les objets disposés comme dans l'expérience est affichée en \ref{fig:exp2}. %\ref{fig:exp2}.

\begin{figure*}
	\begin{center}\includegraphics[width=10cm]{map.png}\end{center}
	\caption{\textbf{Résultat du suivi multi-cibles} - Les points rouges correspondent aux centroïdes des objets segmentés. Chaque croix de couleur représente une estimation de position d'un filtre de Kalman, avec une couleur par filtre. En théorie, un filtre devrait être associé à chaque objet. En noir la trajectoire du robot au cours du temps.}	
	\label{fig:multi_map}
\end{figure*}


\begin{figure*}
	\includegraphics[width=15cm]{multi_recon2.png}
	\caption{\textbf{Reconnaissance Multi-cible} - Quatre des cinq objets présents dans la scène ont été correctement reconnus avec une estimation d'orientation raisonnable. Le première objet (une personne) était trop près du ventilateur, les deux ont donc été confondus dans le multi-tracking. }
	\label{fig:recon}
\end{figure*}

Les résultats de l'expérience montrent que le système fonctionne encore, même dans des cas beaucoup plus complexes. On notera toutefois une chute des taux de reconnaissance. Quelques améliorations proposées dans la section suivante pourraient être utiles pour rendre le système plus robuste:  


\bibliographystyle{plain}
\bibliography{rapport}
\end{document}

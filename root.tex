%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

%\usepackage{graphics} % for pdf, bitmapped graphics files
% The following packages can be found on http:\\www.ctan.org
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\usepackage{latexsym}
%\usepackage{amsthm}
\usepackage{fancyhdr}
%\usepackage{algorithmic}
\usepackage{cite}
\usepackage{array}
\usepackage{enumerate}

\title{\LARGE \bf
An Odometry-Based Approach for Indoor Object Viewpoint Recognition
}

%\title{\LARGE \bf
%An Odometry-Based Reinforcement Approach for Indoor Objects and Viewpoint Recognition
%}

\author{Luigi F. Tedesco$^{1}$, C\'eline Craye$^{2}$, Jean-Fran\c{c}ois Goudou$^{3}$ and David Filliat$^{4}$% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Luigi Franco Tedesco is an undergraduate student with Faculty of Electrical Engineering, Robotics and Artificial Intelligence,
        Thales Service, 91767 Palaiseau, France
        {\tt\small tedesco@ensta.fr}}%
\thanks{$^{2}$C\'eline Craye is with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
\thanks{$^{3}$Jean-Fran\c{c}ois Goudou is with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
\thanks{$^{4}$David Filliat is with the Department of Electrical Engineering, Wright State University,
        Dayton, OH 45435, USA
        {\tt\small b.d.researcher@ieee.org}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Object recognition capability is a essential condition for giving autonomy to mobiles robots in human made environment. However, achieving this goal by means of visually representing objects is a hard task \ref{} and using all possible sources of information is a must. Here we present a procedure to incorporate the notion of continuity and overcome ambiguous points of view. By observing objects from different perspectives bound with a Markovian modeling of the stochastic processes of recognizing each of the objects viewpoint, the algorithm copes with a sparse database, blurred images from motion and object spatial symmetry, to recognize and estimate objects 6-dof pose. A multi-modal Kalman based tracking was also implemented in order to recognize multiple objects simultaneously. The approach was tested in a mobile platform and the comparison between the single viewed and the proposed recognition gave promising results, reinforcing the proposal of blending odometry and recognition. 

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

 The vast majority of the literature focus on single image object visual recognition for helping robots in tasks such as semantic navigation \ref{}, pose estimation for grasping \ref{} and environmental search \ref{}. Typically, a set of features is extracted from a segmented object candidate and, subsequently, compared to a database of priori known objects. Extensive work have been done in order to increase efficiency in each one of the sub-processing steps. Among them : segmentations methods using range cameras, features that describe color and texture \ref{}, geometry \ref{}, contours \ref{}, besides classifiers and matching techniques. Alternatively, a deep neural architecture \ref{} can perform a direct object visual classification after a delicate training phase. However, the classic recognition pipeline seems to be more natural and simple to be implemented with a straight-forward training, still having reasonable results.
 
 Nevertheless, ambiguous viewpoints easily trick visual descriptors reducing its recognition capability. Observing objects sequentially from distinctive points of view seems to be a natural way to deal with the problem. A solution inspired by human behavior for learning new unseen objects has been proposed by \ref{}, using key-frames and the rate of matching features with past frames, to overcome ambiguity in face recognition task. More work have been done to model objects different viewpoints perspectives summarized by Roy and al. \ref{}. 
 
\section{PROPOSED APPROACH}


%L'objectif de notre méthode est d'avoir une reconnaissance multi-vues d'un ou plusieurs objets à la fois, capable d'intégrer le déplacement du robot pour résoudre des ambiguïtés et faux positifs. Pour incorporer les notions de vues et de transition entre elles, on utilise une représentation simple et suffisamment générale basée sur les graphes d'aspect. Le déplacement d'un état à un autre dans ce graphe est ensuite estimé par rapport au déplacement du robot. Ce système est ensuite couplé avec un dispositif de reconnaissance mono-vue classique capable de retrouver la vue la plus probable d'un objet à partir de descripteurs 3D. Une méthode de suivi des objets et un traitement probabiliste de changement de vue étant donné l'information motrice permet enfin d'augmenter le taux de reconnaissance.

\subsection{General archtecture}

Our approach was designed for robots equipped with odometric and RGB-D sensors. 
Informations from these units are sent to a processing module that isolates objects from input images, extracts their characteristics through features, and compare them with a reference database stored in memory. When no matching is found between observation and the current database, a new object can be added to the later, increasing the robot's knowledge about the environment.

The architecture of the system is illustrated in Figure \ref{fig:architecture} and explains the dependencies between the different processing modules and the information flow through them.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{gen_arc.png}
  \caption{General achitecture of the system}
  \label{fig:architecture}
\end{figure*}

More precisely, the processing units takes as input the point cloud from the RGB-D sensor as well as the odometry measure of the two wheels. The first module consists in a segmentation step that cleans up the point cloud by isolating objects of the scene into clusters of points. Those clusters are then sent to the feature extraction module to convert them to discriminative description histograms. Simultaneously, a cartography module converts the odometry data and the segmented object image position into the localization of scene elements in the world referential. Finally, the recognition module uses both the object feature and the evolution of those features compared to the displacement of the robot to retrieve the most likely object and view.

\subsection{Object Segmentation}

The segmentation step aims to differentiate objects from the background of raw images. Stereoscopic and infra-red cameras helped the treatment adding a new dimension to images, allowing segmentation geometrically. In the case of motionless sensors, statical background subtraction approach are typically used for segmentaiton \cite{dai2006prospects}. This is not applicable in our case as the robot is constantly moving in its environment. By making the hypothesis that objects are represented in the scene as cluster of points right above a main plan, approaches such as \textit{Tabletop object detector} \cite{tabletop} and Caron et al. \cite{caron2014neural} determine the main plane's convex hull and search for objects clusters inside it. The later algorithm also removes plans orthogonal to the ground, considered as walls, and is more suitable for finding objects placed on ground level in indoor environments, therefore, explaining our choice for it. Furthermore, in our robot assemble, the camera is always keeping the same orientation towards the floor, in this manner, its plan equation can be estimated once during a calibration step and used during the whole experiment. 

%The segmentation algorithm uses the following steps
%\begin{enumerate}
%\item[0] Calibration to obtain the floor plane equation before experiments, using RANSAC algoritm on an image where floor is the main plane.
%\end{enumerate}
% Then for each frame of the experiment:
%\begin{enumerate}
%\item Floor subtraction from the obtained equation
%\item Points filtering if the distance to the sensor is more than 3 meters
%\item Normal surfaces of the remaining points calculation
%\item Filtering of big planes orthogonal to the ground. They are very likely to be walls
%\item Voxelization of remaining points to speed up processing
%\item Projection of those points on the floor plane
%\item Clustering of the projected points to create object candidates
%\item Elimination of clusters that are too close to borders. They might not be objects, and their missing parts are likely to badly influence the recognition.
%\item Centroid calculation of each remaining cluster (should be the objects of the image)
%\end{enumerate}
%Based on this algorithm, we obtain the position of each object in the camera frame, as well as the associated point cloud and normals.

\subsection{Object representation}

The definition of object used in segmentation is enough for separating them from the environment, however for a more complex classification task, its unique characteristics must be extracted. The first dimension of information relates to its position in the environment. This cartographic data allows the robot to track objects while moving around the scene, consequently associating multiple views of it. A second dimension concerns the objects' intrinsic characteristic capable of differentiating them from one another that can be translated in terms of visual and geometrical features.

\subsubsection{Features} The concept of features tries to transpose those intrinsic characteristics from objects into a lower dimension space. At the same time, they are required to be stable under environmental changes and affine transformations for being truly representative.
Several features were proposed by the literature to incorporate the most diverse characteristics. Among three dimensional geometrical  ones one can state local descriptor SHOT \cite{tombari2010unique} and semi-global CVFH \cite{aldoma2011cad, aldoma2012our} descriptors. The Viewpoint Feature Histogram \ref{} - VFH -  captures in a single histogram the object's geometry by estimating the angular transformation between the normal of each of the its points and the standpoint from where it has been viewed. The interest of using such geometrical features is to explore the ambiguity created from objects' spatial symmetry, even if color and texture descriptors could enhance recognition.

\subsubsection{Aspect-graph}
% Change name
In order to represent objects in the three dimensional space, a model that captures both cartographic and intrinsic components is mandatory. Additionally, object visual characteristics are viewpoint-dependent, thus, requiring an additional treatment that correlates appearance and the necessary movement to transit between them. 

\begin{figure}
 	\centering
	\includegraphics[width=0.3\textwidth]{object_model.png}
  	\caption{Représentation des objets par un modèle polaire}
	\label{fig:graphe_polaire}
\end{figure}

\label{sec:rec_mono}

The above requirements were translated into a polar referential, illustrated in \ref{fig:graphe_polaire} where nodes represent the object aspect from a certain point of view and transitions from nodes can be geometrically deducted. 

Usually, visual features like SIFT are scale-variant as the image resolution plays an important role. Viewpoint information can bias the classification to search for eligible candidates in the database.

%Finally, the ultimate definition of object used in this work is a cluster of points with a certain geometrical arrangement when viewed from a specific point of view.

\subsection{Object Classification}

The viewpoint recognition phase consist on finding the closest element in the database given a test feature to be matched. This classification problem could be solved by classic machine learning techniques. Nevertheless, the work from  Aldoma et al. \cite{Aldoma2012} suggest the use of a K-Nearest Neighbors, K-NN, with a chi-squared distance between histograms for classification. This classifier has the great advantage of having both classification and training stages extremely fast for the intended database size. 

%\begin{equation}
%d(H_1, H_2) = \sum _I \frac{\left(H_1(I)-H_2(I)\right)^2}{H_1(I)}
%\label{eq:chi-square}
%\end{equation}

\begin{figure}
	\includegraphics[width=8.65cm]{mono_recon.png}
	\caption{\textbf{Single-image recognition} - Object identification for two segmented objects - a computer monitor and a small electric fan. The red points represent the floor plan and the ignored white ones exceed the three meters threshold. One could also notice the infra-red shadows created by objects that possible can occlude others}
	\label{fig:mono_recon}   
\end{figure}


\subsection{Multi-object Tracking}

A tracking system is necessary to follow each of the objects on sight. Imprecisions from communication delays between processing modules, segmentation and object centroid calculation call for a multi-modal filtering method for estimating their absolute position. The proposed multi-tracker runs a Kalman filter for each object where each new observation either updates the nearest existing filter or creates a new one if the closest distance to the all filter's states is bigger than a threshold.
 
\subsection {Odometry-Based Multi-view Recognition}

Moving the robot around the environment produce a series of object observations. The idea behind our system is to find consistency between this sequence of possibly mis-recognized objects given a known displacement. In orther words, odometry information can reinforce some viewpoint candidates as they are consistent with the movements prediction.

\subsubsection {Hidden Markov Model}

The first order Markov property hypotheses that a single view  and the robot movement is able to predict the next view of the object, allows the stochastic process of recognizing its views to be modeled as a Markovian process.

Precisely, each hidden state correspond to a view of the object. The probability of transition between states, $\textbf{p}_{ij}^{t}$  is calculated each time a tracked object is observed accordingly to angular displacement of the robot, $\delta_{robot}$, equation \ref{eq:triang}, compared to the necessary displacement to change from view $i$ to $j$, mathematically represented in equation \ref{eq:transmat}. A small probability is attributed to improbable transitions between same object views to reinforce objects recognition rather than pose consistency. Transitions between views of different object are not allowed with zero probability of happening.

\begin{equation}
\delta_{robot} = atan(p_{initial} - p_{obj}) - atan( p_{final} - p_{obj})
\label{eq:triang}
\end{equation}

\begin{equation}
    \textbf{p}_{ij}^{t} = \mathbf{P}( X_t = \theta_i | X_{t-1} = \theta_j ) = 
   \begin{cases}
    1, & \text{if }  \theta_i - \theta_j < \delta_{angle} \\
    \epsilon_1, & \text{otherwise}
\end{cases}
	\label{eq:transmat}
\end{equation}

The emission matrix is also constructed from each new observation. The distance from the K-NN is transformed in a probability distribution of the K neighbors.

\begin{equation}
    \textbf{p}_{i}^e =  
   \begin{cases}
    p_k, & \text{if }  i \in D_k \\
    \epsilon_2, & \text{otherwise}
\end{cases}
	\label{eq:emismat}
\end{equation}

Where $D_k$ is the set of nearest neighbors and $p_k$ is the converted histogram distance into probability.

\subsubsection{Algorithme de Viterbi}

Reste donc à extraire des informations de la modélisation Markovienne proposée.
La séquence d'états la plus vraisemblable qui pourrait avoir géneré
les observations  $y_1,\dots, y_T$, correspond normalement à la séquence d'objets reconnus.
Afin de retrouver cette séquence, aussi appellée chemin, on fait 
appel à la programmation dynamique, et plus spécifiquement à l'algorithme de Viterbi, d'où le nom chemin de Viterbi.
L'algorithme retrouve de façon récursive l'état courant le plus probable, en
prenant en compte seulement les observations jusqu'à un instant donné et son
estimation aux instants antérieurs. Ceci se traduit par les équations \ref{eq:viterbi}

\begin{equation}
  \begin{array}{rcl}
    V_{1,k} &=& \mathrm{P}\big( y_1 \ | \ k \big) \cdot \pi_k \\
    V_{t,k} &=& \max_{x \in S} \left(  \mathrm{P}\big( y_t \ | \ k \big) \cdot a_{x,k} \cdot V_{t-1,x}\right)
  \end{array}
	\label{eq:viterbi}
\end{equation}

Ici, $V_{t,k}$ représente la probabilité que la séquence d'états la plus probable finisse dans l'état $k$, ayant généré les observation à l'instant $t$, tandis que $\pi_i$ représente la probabilité initiale de se retrouver en chaque état. Pour retrouver le chemin de Viterbi, il suffit de trouver le maximum de $V_{t,k}$ :

\begin{equation}
  \begin{array}{rcl}
    x_T &=& \arg\max_{x \in S} (V_{T,x})
  \end{array}
\end{equation}


\section{Experimental results}
The proposed recognition system was deployed in a differential mobile robot, Wifibot V2, embedded with a RGB-D camera, Asus Xtion Pro Live. The algorithm architecture were implemented over ROS using PCL and OpenNi2 libraries. 

In the interest of validating the approach, the robot was initially taught aspects graphs from chosen objects and two sets of experiments were proposed to analyze the efficiency of the algorithm in real scenarios.
 
%In order to succeed, the robot must face inexistent viewpoints due to a sparse database, blurred images from motion and object spatial symmetry

\subsection{Object Database}
First, twenty objects varying in size and form (monitor, boxes, chairs, trashcans, people, ...) were selected to compose the knowledge database of the robot. The aspect graphs of each object weas composed by VFH features from \textit{eight} equally distant viewpoints acquired from positioning the robot around the to be learn object 1.5 meters away. % Each of the feature was labeled with its polar coordinate using 

%For each of them we moved the robot to make a complete circle around them separately. Eights view were obtained from each circle, so that the angular distance between each of them was 45 degrees. The point cloud and associated features were extracted and recorded for each view.
 
\subsection{Odometry Contribution}
% Le mal fonctionnement du tracking a fait qu'on a des arcs de cercle au tour des objects qui commencent de positions aléatoires. Dans le cas où le tracking a marché on a seulement quatre cercles complets
The first experiment consist in a performance comparison between the single image and odometry-base recognition techniques. In other words, this comparison attest whether the proposed architecture is interesting or not, since having the same or worst performance by the cost of adding a complex post-processing and tracking modules is not interesting.

Technically, each object was partially circled by the robot from at least four random initial positions at $0.35 \pm 0.1 m/s$. The robot recorded for each run between 5 and 30 frames at different viewpoints of the object, perhaps inexistent in the database, and try to recognize the object and estimate its pose using both recognition algorithms.

\begin{figure}
	\includegraphics[width=8.65cm]{comp.png}
	
	\caption{\textbf{Résultat de l'évaluation} - Les courbes en pointillés représentent la reconnaissance basée sur une seule image (mono-vue), tandis que les courbes pleines correspondent au système multi-vues. Avec un nombre réduit d'observations, la reconnaissance mono-vue tend à être légèrement plus performante. L'écart se creuse à mesure que ne nombre d'observations augmentent, jusqu'à un écart de 33 \% une fois le tour complet de l'objet effectué, soit 92\% de réussite pour l'estimation de l'objet et 74 \% pour l'estimation de l'orientation. }		
	\label{fig:comp}
\end{figure}

\begin{figure*}
	\includegraphics[height=6cm]{hmm_example.png}
			\caption{\textbf{Expriment typique} - Reconnaissance multi-vue corrige des ambiguïtés malgré la mauvaise segmentation lors de la création de la base.}
	\label{fig:resultats_expe}
\end{figure*}

\subsection{Multi-object recognition}
In a second evaluation, concurrently with the recognition efficiency, the multi-tracking capabilities were put into test. 
\begin{figure}
	\includegraphics[width=8.65cm]{map.png}
	\caption{\textbf{Résultat du suivi multi-cibles} - Les points rouges correspondent aux centroïdes des objets segmentés. Chaque croix de couleur représente une estimation de position d'un filtre de Kalman, avec une couleur par filtre. En théorie, un filtre devrait être associé à chaque objet. En noir la trajectoire du robot au cours du temps.}	
	\label{fig:multi_map}
\end{figure}


La difficulté de l'évaluation vient, premièrement, du fait que la base de donnée a été réalisée avec très peu de vues, ce qui donne lieu à de mauvaises reconnaissance mono-vue lorsque le point de vue observé se situe entre deux vues de la base de données. De plus, la vitesse de déplacement peut générer des images plus floues lors des acquisitions et la modification de l'angle de la caméra\footnote{L'angle entre la base et la tête de la caméra Asus Xtion est facilement modifié.} apportent un obstacle en plus pour le \textit{matching} de descripteurs dans le classificateur K-NN.

Un expérience typique est illustrée dans l'image \ref{fig:resultats_expe} :


La première ligne correspond à la séquence d'images vues par le robot à chaque instant de temps, et donc, l'objet à être reconnu. La seconde ligne, donnée par l'algorithme de reconnaissance, équivaut à la vue la plus probable de l'objet reconnue par le K-plus proches voisins. Il est intéressant remarquer que l'invariance à rotation du descripteur trompe l'estimation de l'orientation en prenant son correspond énantiomorphe dans le premier carré rouge. Autrement, le dos du pinguin étant une grande surface presque plane, il est partiellement retiré par l'étape de segmentation. Ainsi, le nuage de points résultant de ce point de vue n'est pas suffisamment complet pour caractériser correctement l'objet, ce qui induit une mauvaise reconnaissance dans le carré bleu. Au final, on remarque que le traitement apporté par la chaîne de Markov cachée permet de corriger les problèmes d'une base de donnée relativement sparse avec des possibles erreus de segmentation, permettant la correction simultanée de la reconnaissance de l'objet et de son orientation en ligne. 

Le résultat de l'évaluation peut être représenté sous forme d'une courbe : le nombre d'observations pour un même objet en abscisse, par rapport aux pourcentage de reconnaissance d'objet et de vue, pour les reconnaissance mono et multi-vues.



La courbe de la figure \ref{fig:comp} permet de conclure, tout d'abord, que l'algorithme de reconnaissance multi-vues est plus performant que sa correspondante mono-vue lorsque le nombre d'observations augmente suffisamment.\footnote{Le fait que la reconnaissance mono-vue chute avec le nombre de vues peut être liée au fait que pour les premières secondes de l'expérience, le robot est immobile et correctement placé. Lors du déplacement du robot, les prises de vues sont globalement moins bonnes et font chuter le taux de reconnaissance.}. Dans un deuxième temps, on constate que l'estimation de l'orientation de l'objet est plus difficile à estimer que sa reconnaissance, que cela soit pour la reconnaissance mono-vue ou multi-vues.


%\subsection{Robustesse à l'occlusion}

\subsection{Suivi et reconnaissance multi-cibles}

La deuxième expérimentent consiste à placer des objets présents dans la base de données dans une pièce et conduire le robot en faisant en sorte qu'il les regarde  sous plusieurs points de vues différents. Ce scénario est beaucoup plus complexe que celui d'avant. D'abord les objets s'occultent les uns les autres, donnant lieux à de mauvaises segmentations. Ensuite, le suivi des objets est beaucoup plus complexe, car des objets proches peuvent être confondus.

La carte finale donnée par l'algorithme et localisant le robot ainsi que les objets dans le repère absolu est représentée à la figure \ref{fig:multi_map}. Une photo de la pièce aves les objets disposés comme dans l'expérience est affichée en \ref{fig:exp2}. %\ref{fig:exp2}.



\begin{figure*}
	\includegraphics[width=15cm]{multi_recon2.png}
	\caption{\textbf{Reconnaissance Multi-cible} - Quatre des cinq objets présents dans la scène ont été correctement reconnus avec une estimation d'orientation raisonnable. Le première objet (une personne) était trop près du ventilateur, les deux ont donc été confondus dans le multi-tracking. }
	\label{fig:recon}
\end{figure*}

Les résultats de l'expérience montrent que le système fonctionne encore, même dans des cas beaucoup plus complexes. On notera toutefois une chute des taux de reconnaissance. Quelques améliorations proposées dans la section suivante pourraient être utiles pour rendre le système plus robuste:  


\bibliographystyle{plain}
\bibliography{rapport}
\end{document}
